[https://www.safaribooksonline.com/library/view/practical-big-data/9781783554393/6dcb8faa-cd74-4c1d-a762-d467556bd673.xhtml](https://www.gitbook.com/book/nag9s/machine-learning/edit#)



https://www.safaribooksonline.com/library/view/statistics-for-machine/9781788295758/16bf3b8e-a897-45a8-8959-e3d2e1d0ec5b.xhtml

[https://www.safaribooksonline.com/library/view/mastering-predictive-analytics/9781787121393/ch03s05.html](https://www.safaribooksonline.com/library/view/mastering-predictive-analytics/9781787121393/ch03s05.html)

Variable selection \( [The importance of variables feature selection/attribute selection](/the-importance-of-variables-feature-selectionattribute-selection.md) \) is an important process, as it tries to make models simpler to interpret, easier to train, and free of spurious associations by eliminating variables unrelated to the output. This is one possible approach to dealing with the problem of overfitting. In general, we don't expect a model to completely fit our training data; in fact, the problem of overfitting often means that it may be detrimental to our predictive model's accuracy on unseen data if we fit our training data too well. **In this section on regularization**, we'll study an alternative to reducing the number of variables in order to deal with overfitting. **Regularization is essentially the process of introducing an intentional bias or constraint in our training procedure that prevents our coefficients from taking large values. As this is a process that tries to shrink the coefficients, the methods we'll look at are also known as shrinkage methods.**

